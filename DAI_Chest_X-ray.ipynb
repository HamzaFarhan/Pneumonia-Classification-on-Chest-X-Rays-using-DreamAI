{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pneumonia Classification on Chest X-Rays using DreamAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Make sure to change this path to your folder with DreamAI\n",
    "\n",
    "sys.path.insert(0, '/home/farhan/hamza/dreamai/') # Folder with DreamAI\n",
    "\n",
    "# Things below are all included in the dreamai folder\n",
    "\n",
    "import cv_model\n",
    "import utils\n",
    "import obj_utils\n",
    "from dai_imports import*\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your dataset folder, in this case I've named it 'github_chest_xray' should look something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](xray_folder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the necessary paths.\n",
    "\n",
    "All the train,val,test dataframes will be saved in the folder with the dataset. All the best model files will be saved in the current directory by default.\n",
    "\n",
    "'train_name' is the folder with ALL of the images. DreamAI requires all of the images to be in one single folder.\n",
    "\n",
    "If the data is in the Keras format, as is the case with this dataset and most Kaggle datasets, that will be handled in the DataProcessor function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('/home/farhan/hamza/github_chest_xray/')\n",
    "train_name = 'train'\n",
    "val_name = 'val'\n",
    "test_name = 'test'\n",
    "dp_name = 'DP_chest_xray.pkl'\n",
    "train_path = data_path/train_name\n",
    "val_path = data_path/val_name\n",
    "test_path = data_path/test_name\n",
    "dp_path = data_path/dp_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will use DreamAI's data processor to move all the images into one folder and create dataframes for train,val, and test.\n",
    "\n",
    "Everything under the 'Data Setup' heading should be run only once, after that you can comment this part out and start with 'Load and Use the Data'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|              Dream AI              |\n",
      "+------------------------------------+\n",
      "\n",
      "\n",
      "Single-label Classification\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DP = data_processing.DataProcessor(data_path,train_csv=None,tr_name=train_name,setup_data=True)\n",
    "\n",
    "data_processing.save_obj(DP,dp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Use the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our saved DataProcessor object\n",
    "\n",
    "DP = data_processing.load_obj(dp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with information about the data\n",
    "\n",
    "data_dict = DP.data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Label has been changed from 'NORMAL' and 'PNEUMONIA' to 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Img</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_IM-0432-0001.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_IM-0370-0001.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_person1308_bacteria_3286.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_person111_virus_210.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_person437_bacteria_1884.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Img  Label\n",
       "0              train_IM-0432-0001.jpeg      0\n",
       "1              train_IM-0370-0001.jpeg      0\n",
       "2  train_person1308_bacteria_3286.jpeg      1\n",
       "3       train_person111_virus_210.jpeg      1\n",
       "4   train_person437_bacteria_1884.jpeg      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['data_dfs'][train_name].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Img</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>val_NORMAL2-IM-1431-0001.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>val_person1950_bacteria_4881.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>val_NORMAL2-IM-1430-0001.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>val_NORMAL2-IM-1427-0001.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>val_NORMAL2-IM-1438-0001.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Img  Label\n",
       "0      val_NORMAL2-IM-1431-0001.jpeg      0\n",
       "1  val_person1950_bacteria_4881.jpeg      1\n",
       "2      val_NORMAL2-IM-1430-0001.jpeg      0\n",
       "3      val_NORMAL2-IM-1427-0001.jpeg      0\n",
       "4      val_NORMAL2-IM-1438-0001.jpeg      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['data_dfs'][val_name].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_size = (256,256)\n",
    "bs = 32\n",
    "\n",
    "# This is just a precaution for when normalizing the data\n",
    "# This is the percentage of the data to use when calculating the mean and standard deviation\n",
    "# so that it doesn't exceed our memory capacity\n",
    "# 2500000 is a safe number that usually works for me with a 64GB RAM. It will differ for your system\n",
    "\n",
    "stats_percentage = min(1.0,2500000/(image_size[0] * len(data_dict['data_dfs'][train_name])))\n",
    "stats_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloaders\n",
    "# This may take a while because it will normalize according to the whole dataset\n",
    "\n",
    "sets,loaders,sizes = DP.get_data(data_dict, image_size, bs = bs,\n",
    "                                 balance = True, stats_percentage = stats_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 5216, 'val': 16, 'test': 624}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of batches to print progress after\n",
    "\n",
    "print_every = sizes[train_name]//3//bs\n",
    "print_every"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting transfer learning model: self.model set to resnet50\n",
      "Model: resnet50, Setting head: inputs: 2048 hidden:[] outputs: 2\n",
      "Transfer Learning: current best accuracy = 0.000\n",
      "Setting optimizer: SGD\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = cv_model.TransferNetworkImg(model_name='resnet50',model_type='classifier',one_cycle_factor=None,\n",
    "                        optimizer_name='sgd',criterion = nn.CrossEntropyLoss(), device=device,\n",
    "                        best_model_file = 'best_chest_xray_sgd.pth',\n",
    "                        class_names=data_dict['class_names'],num_classes=data_dict['num_classes'],\n",
    "                        dropout_p = 0.5,add_extra=True,\n",
    "                        head = {'num_outputs':data_dict['num_classes'],'model_type':'classifier',\n",
    "                                'layers':[]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment these lines when you want to retrain or just use the model for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.load_state_dict(torch.load('best_chest_xray_sgd.pth'))\n",
    "# net.optimizer.load_state_dict(torch.load('best_chest_xray_sgd_optim.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "net.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding the ideal learning rate.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found it: 0.006812920690579657\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.006812920690579657"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find and set the ideal learning rate\n",
    "\n",
    "lr = net.find_lr(loaders[train_name],plot=True)\n",
    "net.optimizer.param_groups[0]['lr'] = lr\n",
    "net.optimizer.param_groups[0]['lr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1/10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+\n",
      "06:22:27\n",
      "Time elapsed: 42.362 sec\n",
      "Epoch:1/10\n",
      "Batch: 55/163\n",
      "Batch training time: 42.362 sec\n",
      "Batch training loss: 0.181\n",
      "Average training loss: 0.364\n",
      "+----------------------------------------------------------------------+\n",
      "\n",
      "+----------------------------------------------------------------------+\n",
      "06:23:08\n",
      "Time elapsed: 1.383 min\n",
      "Epoch:1/10\n",
      "Batch: 109/163\n",
      "Batch training time: 40.593 sec\n",
      "Batch training loss: 0.067\n",
      "Average training loss: 0.256\n",
      "+----------------------------------------------------------------------+\n",
      "\n",
      "+----------------------------------------------------------------------+\n",
      "06:23:48\n",
      "Time elapsed: 2.059 min\n",
      "Epoch:1/10\n",
      "Batch: 163/163\n",
      "Batch training time: 40.580 sec\n",
      "Batch training loss: 0.087\n",
      "Average training loss: 0.208\n",
      "+----------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "////////////////////////////////////\n",
      "06:23:51\n",
      "Epoch 1/10\n",
      "Validation time: 1.228 sec\n",
      "Epoch training loss: 0.207\n",
      "Epoch validation loss: 0.150\n",
      "Validation accuracy: 100.000\n",
      "\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
      "\n",
      "\n",
      "**********Updating best accuracy**********\n",
      "\n",
      "Previous best: 0.000\n",
      "New best: 100.000\n",
      "\n",
      "******************************************\n",
      "\n",
      "Epoch:  2/10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+\n",
      "06:24:36\n",
      "Time elapsed: 42.120 sec\n",
      "Epoch:2/10\n",
      "Batch: 55/163\n",
      "Batch training time: 42.120 sec\n",
      "Batch training loss: 0.020\n",
      "Average training loss: 0.073\n",
      "+----------------------------------------------------------------------+\n",
      "\n",
      "+----------------------------------------------------------------------+\n",
      "06:25:16\n",
      "Time elapsed: 1.379 min\n",
      "Epoch:2/10\n",
      "Batch: 109/163\n",
      "Batch training time: 40.603 sec\n",
      "Batch training loss: 0.032\n",
      "Average training loss: 0.076\n",
      "+----------------------------------------------------------------------+\n",
      "\n",
      "+----------------------------------------------------------------------+\n",
      "06:25:57\n",
      "Time elapsed: 2.055 min\n",
      "Epoch:2/10\n",
      "Batch: 163/163\n",
      "Batch training time: 40.603 sec\n",
      "Batch training loss: 0.026\n",
      "Average training loss: 0.072\n",
      "+----------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "////////////////////////////////////\n",
      "06:25:59\n",
      "Epoch 2/10\n",
      "Validation time: 1.221 sec\n",
      "Epoch training loss: 0.072\n",
      "Epoch validation loss: 0.171\n",
      "Validation accuracy: 93.750\n",
      "\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
      "\n",
      "Epoch:  3/10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+\n",
      "06:26:41\n",
      "Time elapsed: 42.230 sec\n",
      "Epoch:3/10\n",
      "Batch: 55/163\n",
      "Batch training time: 42.230 sec\n",
      "Batch training loss: 0.015\n",
      "Average training loss: 0.053\n",
      "+----------------------------------------------------------------------+\n",
      "\n",
      "+----------------------------------------------------------------------+\n",
      "06:27:22\n",
      "Time elapsed: 1.382 min\n",
      "Epoch:3/10\n",
      "Batch: 109/163\n",
      "Batch training time: 40.685 sec\n",
      "Batch training loss: 0.011\n",
      "Average training loss: 0.046\n",
      "+----------------------------------------------------------------------+\n",
      "\n",
      "+----------------------------------------------------------------------+\n",
      "06:28:03\n",
      "Time elapsed: 2.060 min\n",
      "Epoch:3/10\n",
      "Batch: 163/163\n",
      "Batch training time: 40.662 sec\n",
      "Batch training loss: 0.083\n",
      "Average training loss: 0.045\n",
      "+----------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "////////////////////////////////////\n",
      "06:28:05\n",
      "Epoch 3/10\n",
      "Validation time: 1.222 sec\n",
      "Epoch training loss: 0.044\n",
      "Epoch validation loss: 0.109\n",
      "Validation accuracy: 100.000\n",
      "\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
      "\n",
      "\n",
      "**********Updating best accuracy**********\n",
      "\n",
      "Previous best: 100.000\n",
      "New best: 100.000\n",
      "\n",
      "******************************************\n",
      "\n",
      "Epoch:  4/10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+\n",
      "06:28:50\n",
      "Time elapsed: 42.297 sec\n",
      "Epoch:4/10\n",
      "Batch: 55/163\n",
      "Batch training time: 42.297 sec\n",
      "Batch training loss: 0.016\n",
      "Average training loss: 0.032\n",
      "+----------------------------------------------------------------------+\n",
      "\n",
      "+----------------------------------------------------------------------+\n",
      "06:29:31\n",
      "Time elapsed: 1.382 min\n",
      "Epoch:4/10\n",
      "Batch: 109/163\n",
      "Batch training time: 40.642 sec\n",
      "Batch training loss: 0.008\n",
      "Average training loss: 0.031\n",
      "+----------------------------------------------------------------------+\n",
      "\n",
      "+----------------------------------------------------------------------+\n",
      "06:30:11\n",
      "Time elapsed: 2.060 min\n",
      "Epoch:4/10\n",
      "Batch: 163/163\n",
      "Batch training time: 40.638 sec\n",
      "Batch training loss: 0.007\n",
      "Average training loss: 0.033\n",
      "+----------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "////////////////////////////////////\n",
      "06:30:13\n",
      "Epoch 4/10\n",
      "Validation time: 1.236 sec\n",
      "Epoch training loss: 0.033\n",
      "Epoch validation loss: 0.035\n",
      "Validation accuracy: 100.000\n",
      "\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
      "\n",
      "\n",
      "**********Updating best accuracy**********\n",
      "\n",
      "Previous best: 100.000\n",
      "New best: 100.000\n",
      "\n",
      "******************************************\n",
      "\n",
      "Epoch:  5/10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+\n",
      "06:30:59\n",
      "Time elapsed: 42.204 sec\n",
      "Epoch:5/10\n",
      "Batch: 55/163\n",
      "Batch training time: 42.204 sec\n",
      "Batch training loss: 0.005\n",
      "Average training loss: 0.024\n",
      "+----------------------------------------------------------------------+\n",
      "\n",
      "+----------------------------------------------------------------------+\n",
      "06:31:39\n",
      "Time elapsed: 1.381 min\n",
      "Epoch:5/10\n",
      "Batch: 109/163\n",
      "Batch training time: 40.648 sec\n",
      "Batch training loss: 0.002\n",
      "Average training loss: 0.025\n",
      "+----------------------------------------------------------------------+\n",
      "\n",
      "+----------------------------------------------------------------------+\n",
      "06:32:20\n",
      "Time elapsed: 2.059 min\n",
      "Epoch:5/10\n",
      "Batch: 163/163\n",
      "Batch training time: 40.662 sec\n",
      "Batch training loss: 0.036\n",
      "Average training loss: 0.023\n",
      "+----------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "////////////////////////////////////\n",
      "06:32:22\n",
      "Epoch 5/10\n",
      "Validation time: 1.227 sec\n",
      "Epoch training loss: 0.023\n",
      "Epoch validation loss: 0.076\n",
      "Validation accuracy: 100.000\n",
      "\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
      "\n",
      "\n",
      "**********Updating best accuracy**********\n",
      "\n",
      "Previous best: 100.000\n",
      "New best: 100.000\n",
      "\n",
      "******************************************\n",
      "\n",
      "Epoch:  6/10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+\n",
      "06:33:07\n",
      "Time elapsed: 42.388 sec\n",
      "Epoch:6/10\n",
      "Batch: 55/163\n",
      "Batch training time: 42.388 sec\n",
      "Batch training loss: 0.011\n",
      "Average training loss: 0.018\n",
      "+----------------------------------------------------------------------+\n",
      "\n",
      "+----------------------------------------------------------------------+\n",
      "06:33:48\n",
      "Time elapsed: 1.384 min\n",
      "Epoch:6/10\n",
      "Batch: 109/163\n",
      "Batch training time: 40.634 sec\n",
      "Batch training loss: 0.079\n",
      "Average training loss: 0.016\n",
      "+----------------------------------------------------------------------+\n",
      "\n",
      "+----------------------------------------------------------------------+\n",
      "06:34:29\n",
      "Time elapsed: 2.061 min\n",
      "Epoch:6/10\n",
      "Batch: 163/163\n",
      "Batch training time: 40.628 sec\n",
      "Batch training loss: 0.004\n",
      "Average training loss: 0.016\n",
      "+----------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "////////////////////////////////////\n",
      "06:34:31\n",
      "Epoch 6/10\n",
      "Validation time: 1.226 sec\n",
      "Epoch training loss: 0.017\n",
      "Epoch validation loss: 0.049\n",
      "Validation accuracy: 100.000\n",
      "\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
      "\n",
      "\n",
      "**********Updating best accuracy**********\n",
      "\n",
      "Previous best: 100.000\n",
      "New best: 100.000\n",
      "\n",
      "******************************************\n",
      "\n",
      "Epoch:  7/10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+\n",
      "06:35:16\n",
      "Time elapsed: 42.094 sec\n",
      "Epoch:7/10\n",
      "Batch: 55/163\n",
      "Batch training time: 42.094 sec\n",
      "Batch training loss: 0.005\n",
      "Average training loss: 0.016\n",
      "+----------------------------------------------------------------------+\n",
      "\n",
      "+----------------------------------------------------------------------+\n",
      "06:35:56\n",
      "Time elapsed: 1.379 min\n",
      "Epoch:7/10\n",
      "Batch: 109/163\n",
      "Batch training time: 40.638 sec\n",
      "Batch training loss: 0.001\n",
      "Average training loss: 0.012\n",
      "+----------------------------------------------------------------------+\n",
      "\n",
      "+----------------------------------------------------------------------+\n",
      "06:36:37\n",
      "Time elapsed: 2.056 min\n",
      "Epoch:7/10\n",
      "Batch: 163/163\n",
      "Batch training time: 40.637 sec\n",
      "Batch training loss: 0.004\n",
      "Average training loss: 0.013\n",
      "+----------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "////////////////////////////////////\n",
      "06:36:39\n",
      "Epoch 7/10\n",
      "Validation time: 1.216 sec\n",
      "Epoch training loss: 0.013\n",
      "Epoch validation loss: 0.015\n",
      "Validation accuracy: 100.000\n",
      "\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
      "\n",
      "\n",
      "**********Updating best accuracy**********\n",
      "\n",
      "Previous best: 100.000\n",
      "New best: 100.000\n",
      "\n",
      "******************************************\n",
      "\n",
      "Epoch:  8/10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+\n",
      "06:37:25\n",
      "Time elapsed: 42.559 sec\n",
      "Epoch:8/10\n",
      "Batch: 55/163\n",
      "Batch training time: 42.559 sec\n",
      "Batch training loss: 0.002\n",
      "Average training loss: 0.007\n",
      "+----------------------------------------------------------------------+\n",
      "\n",
      "+----------------------------------------------------------------------+\n",
      "06:38:05\n",
      "Time elapsed: 1.386 min\n",
      "Epoch:8/10\n",
      "Batch: 109/163\n",
      "Batch training time: 40.597 sec\n",
      "Batch training loss: 0.002\n",
      "Average training loss: 0.008\n",
      "+----------------------------------------------------------------------+\n",
      "\n",
      "+----------------------------------------------------------------------+\n",
      "06:38:46\n",
      "Time elapsed: 2.063 min\n",
      "Epoch:8/10\n",
      "Batch: 163/163\n",
      "Batch training time: 40.628 sec\n",
      "Batch training loss: 0.002\n",
      "Average training loss: 0.009\n",
      "+----------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "////////////////////////////////////\n",
      "06:38:48\n",
      "Epoch 8/10\n",
      "Validation time: 1.222 sec\n",
      "Epoch training loss: 0.009\n",
      "Epoch validation loss: 0.026\n",
      "Validation accuracy: 100.000\n",
      "\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
      "\n",
      "\n",
      "**********Updating best accuracy**********\n",
      "\n",
      "Previous best: 100.000\n",
      "New best: 100.000\n",
      "\n",
      "******************************************\n",
      "\n",
      "Epoch:  9/10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+\n",
      "06:39:33\n",
      "Time elapsed: 42.327 sec\n",
      "Epoch:9/10\n",
      "Batch: 55/163\n",
      "Batch training time: 42.327 sec\n",
      "Batch training loss: 0.001\n",
      "Average training loss: 0.009\n",
      "+----------------------------------------------------------------------+\n",
      "\n",
      "+----------------------------------------------------------------------+\n",
      "06:40:14\n",
      "Time elapsed: 1.383 min\n",
      "Epoch:9/10\n",
      "Batch: 109/163\n",
      "Batch training time: 40.667 sec\n",
      "Batch training loss: 0.002\n",
      "Average training loss: 0.008\n",
      "+----------------------------------------------------------------------+\n",
      "\n",
      "+----------------------------------------------------------------------+\n",
      "06:40:55\n",
      "Time elapsed: 2.061 min\n",
      "Epoch:9/10\n",
      "Batch: 163/163\n",
      "Batch training time: 40.676 sec\n",
      "Batch training loss: 0.061\n",
      "Average training loss: 0.007\n",
      "+----------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "////////////////////////////////////\n",
      "06:40:57\n",
      "Epoch 9/10\n",
      "Validation time: 1.208 sec\n",
      "Epoch training loss: 0.007\n",
      "Epoch validation loss: 0.016\n",
      "Validation accuracy: 100.000\n",
      "\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
      "\n",
      "\n",
      "**********Updating best accuracy**********\n",
      "\n",
      "Previous best: 100.000\n",
      "New best: 100.000\n",
      "\n",
      "******************************************\n",
      "\n",
      "Epoch: 10/10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+\n",
      "06:41:42\n",
      "Time elapsed: 42.317 sec\n",
      "Epoch:10/10\n",
      "Batch: 55/163\n",
      "Batch training time: 42.317 sec\n",
      "Batch training loss: 0.001\n",
      "Average training loss: 0.009\n",
      "+----------------------------------------------------------------------+\n",
      "\n",
      "+----------------------------------------------------------------------+\n",
      "06:42:23\n",
      "Time elapsed: 1.383 min\n",
      "Epoch:10/10\n",
      "Batch: 109/163\n",
      "Batch training time: 40.657 sec\n",
      "Batch training loss: 0.003\n",
      "Average training loss: 0.006\n",
      "+----------------------------------------------------------------------+\n",
      "\n",
      "+----------------------------------------------------------------------+\n",
      "06:43:03\n",
      "Time elapsed: 2.061 min\n",
      "Epoch:10/10\n",
      "Batch: 163/163\n",
      "Batch training time: 40.655 sec\n",
      "Batch training loss: 0.002\n",
      "Average training loss: 0.007\n",
      "+----------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "////////////////////////////////////\n",
      "06:43:05\n",
      "Epoch 10/10\n",
      "Validation time: 1.221 sec\n",
      "Epoch training loss: 0.006\n",
      "Epoch validation loss: 0.105\n",
      "Validation accuracy: 93.750\n",
      "\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
      "\n",
      "\n",
      "Loading best model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net.fit(loaders[train_name],loaders[val_name],epochs=10,print_every=print_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the 'cv_model.TransferNetworkImg' object for later\n",
    "\n",
    "data_processing.save_obj(net,'best_chest_xray_net.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the accuracy quickly hit 100%, but the loss was constantly falling. Let's evaluate our model after training for only 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/farhan/anaconda3/envs/dreamai/lib/python3.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "eval_dict = net.evaluate(loaders[test_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.58333333333333\n"
     ]
    }
   ],
   "source": [
    "print(eval_dict['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NORMAL', 73.07692307692307), ('PNEUMONIA', 99.48717948717949)]\n"
     ]
    }
   ],
   "source": [
    "print(eval_dict['class_accuracies'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      NORMAL       0.98      0.60      0.75       234\n",
      "   PNEUMONIA       0.81      0.99      0.89       390\n",
      "\n",
      "   micro avg       0.85      0.85      0.85       624\n",
      "   macro avg       0.89      0.80      0.82       624\n",
      "weighted avg       0.87      0.85      0.84       624\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Recall is very important in medical use cases and our model has 0.99 recall on x-rays labelled PNEUMONIA\n",
    "\n",
    "print(eval_dict['report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[171  63]\n",
      " [  2 388]]\n"
     ]
    }
   ],
   "source": [
    "print(eval_dict['confusion_matrix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7974358974358975"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dict['roc_auc_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the data distribution.\n",
    "\n",
    "Note: DreamAI uses data augmentation to balance the data while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f2300212710>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_dict['data_dfs'][test_name].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    390\n",
       "0    234\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['data_dfs'][test_name]['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to improve these results such as hyperparameter tuning, training for more epochs, using a different model etc.\n",
    "\n",
    "But this is just to show how easy it is to train a high accuracy image classifier using DreamAI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
